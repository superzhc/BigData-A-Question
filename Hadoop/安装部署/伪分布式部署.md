# 伪分布式部署

Hadoop 可以在单节点上以伪分布式的方式运行，Hadoop 进程以分离的 Java 进程来运行，节点既作为 NameNode 也作为 DataNode，同时，读取的是 HDFS 中的文件。

## 前提

1. 安装 JDK 环境，配置环境变量 `JAVA_HOME`，如配置 `~/.bashrc` 文件如下：
   ```
   export JAVA_HOME=/usr/java/jdk1.8.0_281-amd64
   ```
2. 配置 ssh【即使是单节点也需要，不然会报错】
   ```bash
   #生成密钥、公钥
   ssh-keygen -t rsa #一直点击enter即可
   #把产生的公钥文件放置到authorized_keys文件中
   cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
   #授予权限，这一步必须有
   chmod 0600 ~/.ssh/authorized_keys
   ```

## 创建数据存储目录

1. NameNode 数据存放目录： `/usr/local/data/hadoop/name`
2. DataNode 数据存放目录： `/usr/local/data/hadoop/data`
3. 临时数据存放目录： `/usr/local/data/hadoop/tmp`

```bash
mkdir -p /usr/local/data/hadoop/name
mkdir -p /usr/local/data/hadoop/data
mkdir -p /usr/local/data/hadoop/tmp
```

## 配置 Hadoop

Hadoop 的配置文件位于 `${HADOOP_HOME}/etc/hadoop/` 中，伪分布式需要修改 2 个配置文件 **`core-site.xml`** 和 **`hdfs-site.xml`**。

**`core-site.xml`** 配置如下：

```xml
<configuration>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>file:/usr/local/data/hadoop/tmp</value>
        <description>Abase for other temporary directories.</description>
    </property>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

**`hdfs-site.xml`** 配置如下：

```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:/usr/local/data/hadoop/name</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:/usr/local/data/hadoop/data</value>
    </property>
    <property>
        <name>dfs.permissions.enabled</name>
        <value>FALSE</value>
        <description>关掉权限控制</description>
    </property>
</configuration>
```

## 启动

**格式化 NameNode**（第一次启动需要进行这步操作）

```bash
${HADOOP_HOME}/bin/hdfs namenode -format
```

**启动 NameNode 和 DataNode 守护进程**

```bash
${HADOOP_HOME}/sbin/start-dfs.sh  #start-dfs.sh是个完整的可执行文件，中间没有空格
```

**关闭 NameNode 和 DataNode 守护进程**

```bash
${HADOOP_HOME}/sbin/stop-dfs.sh
```

## 验证

访问 <http://localhost:9870> 即可查看到 HDFS 的网页管理界面

注：*若非部署服务器直接访问，需要查看防火墙是否开启，端口是否开放*