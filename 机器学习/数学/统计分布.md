<!--
 * @Github       : https://github.com/superzhc/BigData-A-Question
 * @Author       : SUPERZHC
 * @CreateDate   : 2020-12-01 11:40:08
 * @LastEditTime : 2020-12-01 18:21:15
 * @Copyright 2020 SUPERZHC
-->
# 统计

## 标准差

**标准差公式**：

$$
\sigma = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (X_{i}- \bar{X})^{2}}
$$

## 众数

枚举的数值中在样本对象中出现次数最多的数值，那就叫做**众数**，如果在样本对象中没有任何一个数值比其他对象多（如所有的数值都只出现一次，或者都出现两次……），这种情况下是不存在众数的，也就是说没有一个数值比其他的数值出现得更多。

## 中位数

中位数，位于中间的数字。

## 欧氏距离

**定义**

> 在一个 N 维度的空间里，求两个点的距离，这个距离肯定是一个大于等于 0 的数字（也就是说没有负距离，最小也就是两个点重合的零距离），那么这个距离需要用两个点在各自维度上的坐标相减，平方后加和再开平方。

一维公式：

$$
c = \sqrt{(x_{1}-x_{2})^{2}}
$$

二维公式：

$$
c = \sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}}
$$

三维公式：

$$
c = \sqrt{(x_{1}-x_{2})^{2}+(y_{1}-y_{2})^{2}+(z_{1}-z{2})^{2}}
$$

同理，四维、五维甚至到 N 维也可进行同样的推导。

## 曼哈顿距离（Manhattan Distance）

对比上面的欧式距离的二维计算公式，曼哈顿距离可以使用如下的计算公式：

$$
c = \left | x_{1}-x_{2} \right | + \left | y_{1}-y_{2} \right |
$$

从公式定义上看，曼哈顿距离一定是一个非负数，距离最小的情况就是两个点重合，距离为 0，这一点和欧氏距离一样。曼哈顿距离和欧氏距离的意义相近，也是为了描述两个点之间的距离，不同的是曼哈顿距离只需要做加减法，这使得计算机在大量的计算过程中代价更低，而且会消除在开平方过程中取近似值而带来的误差。不仅如此，曼哈顿距离在人脱离计算机做计算的时候也会很方便。

## 抽样

## 高斯分布

正态分布（Normal Distribution）又名高斯分布（Gaussian Distribution），是一个在数学、物理及工程等领域都非常重要的概率分布，在统计学的许多方面有着重大的影响力。

**高斯分布的概率密度函数**：

$$
f(x) = \frac{1}{\sqrt{2 \pi } \sigma }exp\left ( - \frac{(x- \mu )^{2}}{2 \sigma^{2}} \right )
$$

exp 指的是自然常数 e 的幂函数，即 e 的多少次幂的概念（e 是一个无理数，也就是无限不循环小数，`e≈2.71828…`）。

高斯分布作为分布特性的一种，首先是用来描述统计对象的，如果统计对象的分布特性符合高斯分布，那么所有针对高斯分布的定理和“经验值”就能够直接套用。而高斯分布本身在自然界的应用是非常广泛的，用一句话解释高斯分布所表现的分布特点就是“一般般的很多，极端的很少”。

## 泊松分布

泊松（Poisson）分布是一种统计与概率学中常见的离散概率分布，由法国数学家西莫恩·德尼·泊松（Simeon-Denis Poisson）在 1838 年发表。泊松分布是概率论中最重要的概念之一。

泊松分布的概率函数如下：

$$
P\left ( X=k \right ) =\frac{\lambda^{k}}{k!}e^{- \lambda} , k=0,1,2,3,\cdots 
$$

泊松分布的参数 $\lambda$ 是单位时间（或单位面积）内随机事件的平均发生率。泊松分布适合于描述单位时间内随机事件发生的次数。其中 $k!$ 是指 k 的阶乘，也就是 $k \times (k-1) \times (k-2) \times \cdots \times 2 \times 1$，k 取非负整数。

## 伯努利分布

伯努利分布（Bernoulli Distribution）是一种离散分布，在概率学中非常常用，有两种可能的结果，1 表示成功，出现的概率为 p（其中 `0<p<1`）；0 表示失败，出现的概率为 `q=1-p`。这很好理解，除去成功都是失败，p`是成功的概率，概率 100% 减去 p 就是失败的概率。

伯努利分布的分布律如下：

$$
P_{n} = \begin{Bmatrix}
p & n=1 \\ 
1-p & n=0 
\end{Bmatrix}
$$

伯努利分布的应用需满足以下条件：

- 各次试验中的事件是互相独立的，每一次 `n=1` 和 `n=0` 的概率分别为 p 和 q
- 每次试验都只有两种结果，即 `n=0`，或 `n=1`。

如果不满足这两个条件，则分布不是伯努利分布。