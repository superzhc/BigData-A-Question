<!--
 * @Github       : https://github.com/superzhc/BigData-A-Question
 * @Author       : SUPERZHC
 * @CreateDate   : 2020-12-03 00:22:22
 * @LastEditTime : 2020-12-03 08:30:16
 * @Copyright 2020 SUPERZHC
-->
# 模型评估与选择

## 经验误差与过拟合

通常把分类错误的样本数占样本总数的比例称为 **错误率**（error rate），即如果在 $m$ 个样本中有 $a$ 个样本分类错误，则错误率 $E=a/m$；相应的，$1-a/m$ 称为 **精度**（accuracy），即 **精度 = 1 - 错误率**。更一般地，把学习器的实际预测输出与样本的真实输出之间的差异称为 **误差**（error），学习器在训练集上的误差称为 **训练误差**（training error）或 **经验误差**（empirical error），在新样本上的误差称为 **泛化误差**（generalization error）。

因为事先并不知道新样本是什么样的，导致实际上能做的只是努力使经验误差最小化，但这样很可能已经把训练样本身的一些特点当作了所有潜在样本都会具有的一般性质，这样会导致泛化性能下降。这种现象在机器学习中称为 **过拟合**（overfitting），与过拟合相对的是 **欠拟合**（underfitting），这是指对训练样本的一般性质尚未学好。

## 评估方法

通常，可通过实验测试来对学习器的泛化误差进行评估并进而做出选择。为此，需使用一个 **测试集**（testing set）来测试学习器对新样本的判别能力，然后以测试集上的 **测试误差**（testing error）作为泛化误差的近似。通常假设测试样本也是从样本真实分布中独立同分布采样而得。但需注意的是，测试集应该尽可能与训练集互斥，即测试样本尽量不在训练集中出现、未在训练过程中使用过。

当只有一个包含 $m$ 个样例的数据集 $D = \left \{(x_{1},y_{1}),(x_{2},y_{2}), \cdots ,(x_{m},y_{m}) \right \}$，既要训练，又要测试。可以通过对 D 进行适当的处理，从中产生出训练集 S 和测试集 T。有如下几种常见的做法。

### 留出法

**留出法**（hold-out）直接将数据集 D 划分为两个互斥的集合，其中一个集合作为训练集 S，另一个作为测试集 T，即 $D = S \cup T$，$S \cap T = \varnothing$。在 S 上训练出模型后，用 T 来评估其测试误差，作为泛化误差的估计。

### 交叉验证法

### 自助法